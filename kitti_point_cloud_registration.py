import numpy as np
import matplotlib.pyplot as plt
import open3d as o3d
import os
import glob
from scipy.spatial.transform import Rotation
import time
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.colors as mcolors
from sklearn.neighbors import NearestNeighbors
import pickle
import json
import argparse
from pathlib import Path

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False

class ImprovedKITTIPointCloudMapper:
    def __init__(self, base_path, dataset_prefix, pose_file=None):
        self.base_path = Path(base_path)
        self.date = dataset_prefix
        self.drive = f'{dataset_prefix}_sync'
        
        # ë°ì´í„° ê²½ë¡œ ì„¤ì •
        self.data_path = self.base_path / 'data' / self.drive
        self.calib_path = self.base_path / 'calibration' / self.date
        
        # íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        self.velodyne_path = self.data_path / 'velodyne_points' / 'data'
        self.oxts_path = self.data_path / 'oxts' / 'data'
        
        self.velodyne_files = sorted(glob.glob(str(self.velodyne_path / '*.bin')))
        self.oxts_files = sorted(glob.glob(str(self.oxts_path / '*.txt')))
        
        # ì‚¬ì „ ê³„ì‚°ëœ pose ë¡œë“œ
        self.pose_file = pose_file
        self.precomputed_poses = None
        if pose_file:
            self.load_precomputed_poses(pose_file)
        
        # GT vs ICP ë¹„êµ
        self.gt_aligned_points = []
        self.gt_aligned_colors = []
        self.icp_aligned_points = []
        self.icp_aligned_colors = []
        
        self.frame_poses_gt = []
        self.frame_poses_icp = []
        self.initial_pose = None
        
        # ë§µí•‘ í†µê³„
        self.total_points = 0
        self.processing_times = []
        
        # ICP ê´€ë ¨ (ì‚¬ì „ ê³„ì‚°ëœ poseê°€ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
        self.icp_cumulative_pose = np.eye(4)
        self.prev_features = None
        self.keyframes = []
        self.keyframe_poses = []
        self.keyframe_features = []
    
    def load_precomputed_poses(self, pose_file):
        """ì‚¬ì „ ê³„ì‚°ëœ pose íŒŒì¼ ë¡œë“œ"""
        try:
            print(f"\nì‚¬ì „ ê³„ì‚°ëœ pose íŒŒì¼ ë¡œë“œ ì¤‘: {pose_file}")
            with open(pose_file, 'rb') as f:
                self.precomputed_poses = pickle.load(f)
            
            # ë””ë²„ê¹…: pose ë°ì´í„° êµ¬ì¡° í™•ì¸
            print("\nPose ë°ì´í„° êµ¬ì¡°:")
            print(f"  â€¢ í‚¤ ëª©ë¡: {list(self.precomputed_poses.keys())}")
            if 'gt_poses' in self.precomputed_poses:
                print(f"  â€¢ GT poses ê°œìˆ˜: {len(self.precomputed_poses['gt_poses'])}")
                if len(self.precomputed_poses['gt_poses']) > 0:
                    print(f"  â€¢ ì²« ë²ˆì§¸ GT pose ì˜ˆì‹œ:\n{self.precomputed_poses['gt_poses'][0]}")
            if 'icp_poses' in self.precomputed_poses:
                print(f"  â€¢ ICP poses ê°œìˆ˜: {len(self.precomputed_poses['icp_poses'])}")
                if len(self.precomputed_poses['icp_poses']) > 0:
                    print(f"  â€¢ ì²« ë²ˆì§¸ ICP pose ì˜ˆì‹œ:\n{self.precomputed_poses['icp_poses'][0]}")
            
            print("âœ… Pose íŒŒì¼ ë¡œë“œ ì„±ê³µ!")
            print(f"  â€¢ ì´ í”„ë ˆì„: {len(self.precomputed_poses.get('gt_poses', []))}")
            print(f"  â€¢ ì•Œê³ ë¦¬ì¦˜: {self.precomputed_poses.get('metadata', {}).get('algorithm', 'Unknown')}")
            print(f"  â€¢ ìƒì„± ì‹œê°„: {self.precomputed_poses.get('metadata', {}).get('timestamp', 'Unknown')}")
            
        except Exception as e:
            print(f"âŒ Pose íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.precomputed_poses = None
    
    def get_pose_from_precomputed(self, frame_idx, pose_type='gt'):
        """ì‚¬ì „ ê³„ì‚°ëœ poseì—ì„œ íŠ¹ì • í”„ë ˆì„ì˜ poseë¥¼ ê°€ì ¸ì˜´"""
        if self.precomputed_poses is None:
            return None
            
        try:
            if pose_type == 'gt':
                return self.precomputed_poses['gt_poses'][frame_idx]
            elif pose_type == 'icp':
                return self.precomputed_poses['fused_poses'][frame_idx]  # 'icp_poses' -> 'fused_poses'
            else:
                print(f"ê²½ê³ : ì•Œ ìˆ˜ ì—†ëŠ” pose íƒ€ì…: {pose_type}")
                return None
        except (KeyError, IndexError) as e:
            print(f"ê²½ê³ : í”„ë ˆì„ {frame_idx}ì˜ poseë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            return None
    
    def load_velodyne_points(self, file_path):
        """ë¼ì´ë‹¤ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¡œë“œ"""
        points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)
        return points
    
    def load_oxts_data(self, file_path):
        """GPS/IMU ë°ì´í„° ë¡œë“œ"""
        with open(file_path, 'r') as f:
            data = f.readline().strip().split()
        
        return {
            'lat': float(data[0]),
            'lon': float(data[1]),
            'alt': float(data[2]),
            'roll': float(data[3]),
            'pitch': float(data[4]),
            'yaw': float(data[5]),
            'vn': float(data[6]),
            've': float(data[7]),
            'vf': float(data[8])
        }
    
    def lat_lon_to_xyz(self, lat, lon, alt, ref_lat, ref_lon, ref_alt):
        """ìœ„ë„/ê²½ë„ë¥¼ XYZ ì¢Œí‘œë¡œ ë³€í™˜"""
        x = (lon - ref_lon) * 111320 * np.cos(np.radians(ref_lat))
        y = (lat - ref_lat) * 111320
        z = alt - ref_alt
        return np.array([x, y, z])
    
    def get_pose_from_oxts(self, frame_idx):
        """OXTS ë°ì´í„°ë¡œë¶€í„° GT pose ê³„ì‚°"""
        oxts_data = self.load_oxts_data(self.oxts_files[frame_idx])
        
        if self.initial_pose is None:
            self.initial_pose = {
                'lat': oxts_data['lat'],
                'lon': oxts_data['lon'],
                'alt': oxts_data['alt']
            }
        
        # ìœ„ì¹˜ ê³„ì‚°
        position = self.lat_lon_to_xyz(
            oxts_data['lat'], oxts_data['lon'], oxts_data['alt'],
            self.initial_pose['lat'], self.initial_pose['lon'], self.initial_pose['alt']
        )
        
        # íšŒì „ ê³„ì‚°
        rotation = Rotation.from_euler('xyz', [
            oxts_data['roll'],
            oxts_data['pitch'],
            oxts_data['yaw']
        ])
        
        # 4x4 ë³€í™˜ í–‰ë ¬ ìƒì„±
        pose_matrix = np.eye(4)
        pose_matrix[:3, :3] = rotation.as_matrix()
        pose_matrix[:3, 3] = position
        
        return pose_matrix
    
    def extract_features(self, pcd):
        """í¬ì¸íŠ¸ í´ë¼ìš°ë“œì—ì„œ íŠ¹ì§•ì  ì¶”ì¶œ"""
        # FPFH íŠ¹ì§• ê³„ì‚°
        radius_normal = 2.0
        radius_feature = 5.0
        
        # ë²•ì„  ë²¡í„° ê³„ì‚°
        pcd.estimate_normals(o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))
        
        # FPFH íŠ¹ì§• ê³„ì‚°
        fpfh = o3d.pipelines.registration.compute_fpfh_feature(
            pcd, o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100)
        )
        
        return fpfh
    
    def preprocess_point_cloud(self, points, voxel_size=0.1):
        """ê³ ê¸‰ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì „ì²˜ë¦¬"""
        # Open3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(points[:, :3])
        
        # 1. ì§€ë©´ ì œê±° (RANSAC ì‚¬ìš©)
        plane_model, inliers = pcd.segment_plane(distance_threshold=0.3, ransac_n=3, num_iterations=1000)
        pcd = pcd.select_by_index(inliers, invert=True)
        
        # 2. í†µê³„ì  ì•„ì›ƒë¼ì´ì–´ ì œê±° (ë” ì—„ê²©í•˜ê²Œ)
        pcd, _ = pcd.remove_statistical_outlier(nb_neighbors=20, std_ratio=2.0)
        
        # 3. ë‹¤ìš´ìƒ˜í”Œë§ (ë” ì‘ì€ ë³µì…€)
        pcd = pcd.voxel_down_sample(voxel_size)
        
        # 4. ë°˜ì§€ë¦„ ê¸°ë°˜ ì•„ì›ƒë¼ì´ì–´ ì œê±°
        pcd, _ = pcd.remove_radius_outlier(nb_points=16, radius=0.5)
        
        return pcd
    
    def multi_scale_icp(self, source, target, scales=[0.5, 0.2, 0.1]):
        """ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ICP ì •í•©"""
        current_transformation = np.eye(4)
        
        for scale in scales:
            # ìŠ¤ì¼€ì¼ì— ë”°ë¥¸ ë‹¤ìš´ìƒ˜í”Œë§
            voxel_size = scale
            source_down = source.voxel_down_sample(voxel_size)
            target_down = target.voxel_down_sample(voxel_size)
            
            # ê±°ë¦¬ ì„ê³„ê°’ë„ ìŠ¤ì¼€ì¼ì— ë§ê²Œ ì¡°ì •
            max_correspondence_distance = scale * 2.0
            
            # Point-to-Plane ICP ìˆ˜í–‰
            result = o3d.pipelines.registration.registration_icp(
                source_down, target_down,
                max_correspondence_distance=max_correspondence_distance,
                init=current_transformation,
                estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPlane(),
                criteria=o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=100)
            )
            
            current_transformation = result.transformation
            
            # ìˆ˜ë ´ í’ˆì§ˆ í™•ì¸
            if result.fitness < 0.3:  # í’ˆì§ˆì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì´ì „ ë³€í™˜ ìœ ì§€
                print(f"ICP í’ˆì§ˆì´ ë‚®ìŒ (fitness: {result.fitness:.3f})")
                break
        
        return current_transformation
    
    def feature_based_initial_alignment(self, source, target, source_fpfh, target_fpfh):
        """íŠ¹ì§• ê¸°ë°˜ ì´ˆê¸° ì •ë ¬"""
        # RANSACì„ ì‚¬ìš©í•œ íŠ¹ì§• ë§¤ì¹­
        distance_threshold = 1.5
        result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(
            source, target, source_fpfh, target_fpfh,
            mutual_filter=True,
            max_correspondence_distance=distance_threshold,
            estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(False),
            ransac_n=3,
            checkers=[
                o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),
                o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(distance_threshold)
            ],
            criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999)
        )
        
        return result.transformation
    
    def detect_loop_closure(self, current_features, frame_idx, threshold=0.85):
        """ë£¨í”„ í´ë¡œì € ê²€ì¶œ"""
        if len(self.keyframe_features) < 10:  # ìµœì†Œ 10ê°œ í‚¤í”„ë ˆì„ í•„ìš”
            return None, -1
        
        # í˜„ì¬ í”„ë ˆì„ê³¼ ì´ì „ í‚¤í”„ë ˆì„ë“¤ ê°„ì˜ íŠ¹ì§• ìœ ì‚¬ë„ ê³„ì‚°
        for i, kf_features in enumerate(self.keyframe_features[:-5]):  # ìµœê·¼ 5ê°œëŠ” ì œì™¸
            if current_features is None or kf_features is None:
                continue
                
            # FPFH íŠ¹ì§• ë¹„êµ
            try:
                current_feat = np.asarray(current_features.data).T
                kf_feat = np.asarray(kf_features.data).T
                
                # ìµœê·¼ì ‘ ì´ì›ƒ ë§¤ì¹­
                nbrs = NearestNeighbors(n_neighbors=1).fit(kf_feat)
                distances, indices = nbrs.kneighbors(current_feat)
                
                # ë§¤ì¹­ ë¹„ìœ¨ ê³„ì‚°
                good_matches = np.sum(distances < 0.1) / len(distances)
                
                if good_matches > threshold:
                    return self.keyframe_poses[i], i
            except:
                continue
        
        return None, -1
    
    def improved_icp_pose_estimation(self, curr_points, prev_points, prev_features=None):
        """ê°œì„ ëœ ICP ê¸°ë°˜ pose ì¶”ì •"""
        if prev_points is None:
            return np.eye(4), None
        
        # 1. í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì „ì²˜ë¦¬
        source_pcd = self.preprocess_point_cloud(prev_points, voxel_size=0.1)
        target_pcd = self.preprocess_point_cloud(curr_points, voxel_size=0.1)
        
        if len(source_pcd.points) < 100 or len(target_pcd.points) < 100:
            return np.eye(4), None
        
        # 2. íŠ¹ì§• ì¶”ì¶œ
        try:
            source_fpfh = self.extract_features(source_pcd)
            target_fpfh = self.extract_features(target_pcd)
            
            # 3. íŠ¹ì§• ê¸°ë°˜ ì´ˆê¸° ì •ë ¬
            initial_transform = self.feature_based_initial_alignment(
                source_pcd, target_pcd, source_fpfh, target_fpfh
            )
        except:
            initial_transform = np.eye(4)
            target_fpfh = None
        
        # 4. ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ ICPë¡œ ì •ë°€ ì •í•©
        try:
            final_transform = self.multi_scale_icp(source_pcd, target_pcd, scales=[0.5, 0.2, 0.1])
            
            # ì´ˆê¸° ì •ë ¬ê³¼ ì •ë°€ ì •í•© ê²°í•©
            if not np.allclose(initial_transform, np.eye(4)):
                combined_transform = final_transform @ initial_transform
            else:
                combined_transform = final_transform
                
        except:
            combined_transform = initial_transform
        
        # 5. ë³€í™˜ì´ ë„ˆë¬´ í´ ê²½ìš° í•„í„°ë§ (ì´ìƒì¹˜ ì œê±°)
        translation_norm = np.linalg.norm(combined_transform[:3, 3])
        rotation_angle = np.arccos(np.clip((np.trace(combined_transform[:3, :3]) - 1) / 2, -1, 1))
        
        # í•©ë¦¬ì ì¸ ë²”ìœ„ ë‚´ì˜ ë³€í™˜ë§Œ í—ˆìš© (ë” ì—„ê²©í•˜ê²Œ)
        if translation_norm > 10.0 or rotation_angle > np.pi/6:  # 10m ì´ìƒ ë˜ëŠ” 30ë„ ì´ìƒ íšŒì „ì€ ê±°ë¶€
            print(f"ë¹„í˜„ì‹¤ì ì¸ ë³€í™˜ ê±°ë¶€: translation={translation_norm:.2f}m, rotation={np.degrees(rotation_angle):.2f}Â°")
            combined_transform = np.eye(4)
        
        return combined_transform, target_fpfh
    
    def filter_point_cloud(self, points, intensity_threshold=0.1, distance_range=(3.0, 80.0)):
        """í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í•„í„°ë§"""
        # ê°•ë„ í•„í„°ë§
        intensity_mask = points[:, 3] > intensity_threshold
        
        # ê±°ë¦¬ í•„í„°ë§
        distances = np.linalg.norm(points[:, :3], axis=1)
        distance_mask = (distances >= distance_range[0]) & (distances <= distance_range[1])
        
        # ë†’ì´ í•„í„°ë§ (ì§€ë©´ì—ì„œ ë„ˆë¬´ ë†’ê±°ë‚˜ ë‚®ì€ ì  ì œê±°)
        height_mask = (points[:, 2] > -2.0) & (points[:, 2] < 8.0)
        
        # ì „ì²´ ë§ˆìŠ¤í¬ ì ìš©
        valid_mask = intensity_mask & distance_mask & height_mask
        filtered_points = points[valid_mask]
        
        return filtered_points
    
    def generate_point_colors(self, points, frame_idx, total_frames, color_scheme='time'):
        """í”„ë ˆì„ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒ ìƒì„±"""
        if color_scheme == 'time':
            # ì‹œê°„ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€í™” (íŒŒë€ìƒ‰ -> ë…¹ìƒ‰ -> ë¹¨ê°„ìƒ‰)
            color_ratio = frame_idx / max(total_frames - 1, 1)
            hue = (1.0 - color_ratio) * 0.7  # íŒŒë€ìƒ‰(0.7)ì—ì„œ ë¹¨ê°„ìƒ‰(0.0)ìœ¼ë¡œ
            saturation = 0.8
            value = 0.9
            rgb_color = mcolors.hsv_to_rgb([hue, saturation, value])
            # RGB ê°’ì´ 0-1 ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ í´ë¦¬í•‘
            rgb_color = np.clip(rgb_color, 0, 1)
        else:
            rgb_color = [0.7, 0.7, 0.7]  # ê¸°ë³¸ íšŒìƒ‰
        
        # ëª¨ë“  ì ì— ê°™ì€ ìƒ‰ìƒ ì ìš©
        colors = np.tile(rgb_color, (len(points), 1))
        return colors
    
    def transform_points_to_world(self, points, pose_matrix):
        """í¬ì¸íŠ¸ë¥¼ ì›”ë“œ ì¢Œí‘œê³„ë¡œ ë³€í™˜"""
        # í¬ì¸íŠ¸ë¥¼ homogeneous ì¢Œí‘œë¡œ ë³€í™˜
        points_homo = np.ones((len(points), 4))
        points_homo[:, :3] = points[:, :3]
        
        # ë³€í™˜ ì ìš©
        points_world = (pose_matrix @ points_homo.T).T
        
        return points_world[:, :3]
    
    def create_comparison_maps(self, start_idx=0, end_idx=None, step=2):
        """ì‚¬ì „ ê³„ì‚°ëœ poseë¥¼ ì‚¬ìš©í•œ GT vs ICP ë¹„êµ"""
        if end_idx is None:
            end_idx = min(len(self.velodyne_files), 100)
        
        # ì‚¬ì „ ê³„ì‚°ëœ poseê°€ ìˆëŠ”ì§€ í™•ì¸
        if self.precomputed_poses:
            max_frames = self.precomputed_poses['metadata']['total_frames']
            end_idx = min(end_idx, max_frames)
            print(f"ì‚¬ì „ ê³„ì‚°ëœ pose ì‚¬ìš© - ìµœëŒ€ í”„ë ˆì„: {max_frames}")
        
        print(f"í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë§µ ìƒì„± ì‹œì‘...")
        print(f"ì²˜ë¦¬í•  í”„ë ˆì„: {start_idx}ë¶€í„° {end_idx}ê¹Œì§€ (step: {step})")
        
        prev_points = None
        prev_features = None
        
        for i, frame_idx in enumerate(range(start_idx, end_idx, step)):
            start_time = time.time()
            
            # í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë¡œë“œ
            points = self.load_velodyne_points(self.velodyne_files[frame_idx])
            filtered_points = self.filter_point_cloud(points)
            
            # 1. GT pose ê¸°ë°˜ ì •í•© (ì‚¬ì „ ê³„ì‚°ëœ pose ì‚¬ìš©)
            if self.precomputed_poses:
                gt_pose = self.get_pose_from_precomputed(frame_idx, 'gt')
                if gt_pose is None:
                    # fallback to real-time calculation
                    gt_pose = self.get_pose_from_oxts(frame_idx)
            else:
                gt_pose = self.get_pose_from_oxts(frame_idx)
            
            self.frame_poses_gt.append(gt_pose)
            gt_world_points = self.transform_points_to_world(filtered_points, gt_pose)
            gt_colors = self.generate_point_colors(filtered_points, i, (end_idx - start_idx) // step, 'time')
            
            self.gt_aligned_points.append(gt_world_points)
            self.gt_aligned_colors.append(gt_colors)
            
            # 2. ì¶”ì •ëœ pose ê¸°ë°˜ ì •í•© (ì‚¬ì „ ê³„ì‚°ëœ pose ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ICP)
            if self.precomputed_poses:
                # ì‚¬ì „ ê³„ì‚°ëœ ì¶”ì • pose ì‚¬ìš©
                estimated_pose = self.get_pose_from_precomputed(frame_idx, 'icp')
                if estimated_pose is None:
                    estimated_pose = np.eye(4)
                self.frame_poses_icp.append(estimated_pose)
                estimated_world_points = self.transform_points_to_world(filtered_points, estimated_pose)
                estimated_colors = self.generate_point_colors(filtered_points, i, (end_idx - start_idx) // step, 'time')
                
                self.icp_aligned_points.append(estimated_world_points)
                self.icp_aligned_colors.append(estimated_colors)
                
            else:
                # ì‹¤ì‹œê°„ ICP ìˆ˜í–‰ (ê¸°ì¡´ ë°©ì‹)
                if prev_points is not None:
                    icp_relative_pose, curr_features = self.improved_icp_pose_estimation(
                        filtered_points, prev_points, prev_features
                    )
                    
                    # ë£¨í”„ í´ë¡œì € ê²€ì¶œ
                    if curr_features is not None and i % 5 == 0:
                        loop_pose, loop_idx = self.detect_loop_closure(curr_features, frame_idx)
                        if loop_pose is not None:
                            print(f"ë£¨í”„ í´ë¡œì € ê²€ì¶œ! í”„ë ˆì„ {frame_idx}ì—ì„œ í‚¤í”„ë ˆì„ {loop_idx}ì™€ ë§¤ì¹­")
                            self.icp_cumulative_pose = loop_pose
                        
                        self.keyframes.append(frame_idx)
                        self.keyframe_poses.append(self.icp_cumulative_pose.copy())
                        self.keyframe_features.append(curr_features)
                    
                    self.icp_cumulative_pose = self.icp_cumulative_pose @ icp_relative_pose
                    prev_features = curr_features
                else:
                    curr_features = None
                
                self.frame_poses_icp.append(self.icp_cumulative_pose.copy())
                icp_world_points = self.transform_points_to_world(filtered_points, self.icp_cumulative_pose)
                icp_colors = self.generate_point_colors(filtered_points, i, (end_idx - start_idx) // step, 'time')
                
                self.icp_aligned_points.append(icp_world_points)
                self.icp_aligned_colors.append(icp_colors)
            
            # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
            processing_time = time.time() - start_time
            self.processing_times.append(processing_time)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if i % 10 == 0:
                pose_source = "ì‚¬ì „ê³„ì‚°" if self.precomputed_poses else "ì‹¤ì‹œê°„"
                print(f"í”„ë ˆì„ {frame_idx} ì²˜ë¦¬ ì™„ë£Œ ({pose_source}) - "
                      f"í¬ì¸íŠ¸: {len(filtered_points):,}ê°œ, "
                      f"ì‹œê°„: {processing_time:.3f}ì´ˆ")
            
            prev_points = filtered_points
            self.total_points += len(filtered_points)
        
        print(f"\në§µ ìƒì„± ì™„ë£Œ! ì´ {self.total_points:,}ê°œ í¬ì¸íŠ¸")
        if not self.precomputed_poses:
            print(f"í‚¤í”„ë ˆì„ ìˆ˜: {len(self.keyframes)}")
    
    def create_optimized_point_clouds(self, voxel_size=0.2):
        """ìµœì í™”ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±"""
        print("í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìµœì í™” ì‹œì‘...")
        
        # ì‚¬ì „ ê³„ì‚°ëœ pose ë°ì´í„°ì˜ ë²”ìœ„ í™•ì¸
        if self.precomputed_poses:
            max_gt_frames = len(self.precomputed_poses.get('gt_poses', []))
            max_icp_frames = len(self.precomputed_poses.get('fused_poses', []))
            max_available_frames = min(max_gt_frames, max_icp_frames, len(self.velodyne_files))
            print(f"  â€¢ Velodyne íŒŒì¼ ìˆ˜: {len(self.velodyne_files)}")
            print(f"  â€¢ GT pose ìˆ˜: {max_gt_frames}")
            print(f"  â€¢ ICP pose ìˆ˜: {max_icp_frames}")
            print(f"  â€¢ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœëŒ€ í”„ë ˆì„: {max_available_frames}")
        else:
            max_available_frames = len(self.velodyne_files)
        
        # GT í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
        gt_points = []
        for frame_idx in range(max_available_frames):
            points = self.load_velodyne_points(self.velodyne_files[frame_idx])
            pose = self.get_pose_from_precomputed(frame_idx, pose_type='gt')
            if pose is not None:
                transformed_points = self.transform_points_to_world(points, pose)
                gt_points.append(transformed_points)
            else:
                print(f"âš ï¸ í”„ë ˆì„ {frame_idx}ì˜ GT pose ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if not gt_points:
            print("âŒ GT í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pose ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        gt_points = np.vstack(gt_points)
        
        # ì¶”ì •ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±
        estimated_points = []
        for frame_idx in range(max_available_frames):
            points = self.load_velodyne_points(self.velodyne_files[frame_idx])
            pose = self.get_pose_from_precomputed(frame_idx, pose_type='icp')
            if pose is not None:
                transformed_points = self.transform_points_to_world(points, pose)
                estimated_points.append(transformed_points)
            else:
                print(f"âš ï¸ í”„ë ˆì„ {frame_idx}ì˜ ì¶”ì • pose ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if not estimated_points:
            print("âŒ ì¶”ì •ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. pose ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None, None
            
        estimated_points = np.vstack(estimated_points)
        
        # ë‹¤ìš´ìƒ˜í”Œë§ìœ¼ë¡œ ë©”ëª¨ë¦¬ ë° ì„±ëŠ¥ ìµœì í™”
        print(f"ë‹¤ìš´ìƒ˜í”Œë§ ì‹œì‘ (ë³µì…€ í¬ê¸°: {voxel_size}m)...")
        
        # GT í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë‹¤ìš´ìƒ˜í”Œë§
        gt_pcd_temp = o3d.geometry.PointCloud()
        gt_pcd_temp.points = o3d.utility.Vector3dVector(gt_points[:, :3])
        gt_pcd_temp = gt_pcd_temp.voxel_down_sample(voxel_size)
        gt_points_downsampled = np.asarray(gt_pcd_temp.points)
        
        # ì¶”ì •ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë‹¤ìš´ìƒ˜í”Œë§
        est_pcd_temp = o3d.geometry.PointCloud()
        est_pcd_temp.points = o3d.utility.Vector3dVector(estimated_points[:, :3])
        est_pcd_temp = est_pcd_temp.voxel_down_sample(voxel_size)
        estimated_points_downsampled = np.asarray(est_pcd_temp.points)
        
        print(f"ë‹¤ìš´ìƒ˜í”Œë§ ì™„ë£Œ:")
        print(f"  â€¢ GT í¬ì¸íŠ¸: {len(gt_points):,} â†’ {len(gt_points_downsampled):,}")
        print(f"  â€¢ ì¶”ì • í¬ì¸íŠ¸: {len(estimated_points):,} â†’ {len(estimated_points_downsampled):,}")
        
        # Open3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± (ë‹¤ìš´ìƒ˜í”Œë§ëœ ë²„ì „)
        self.gt_pcd = o3d.geometry.PointCloud()
        self.gt_pcd.points = o3d.utility.Vector3dVector(gt_points_downsampled)
        
        # ì‹œê°„ì— ë”°ë¥¸ ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ ìƒì„±
        num_points = len(gt_points_downsampled)
        colors = np.zeros((num_points, 3))
        for i in range(num_points):
            # ì‹œê°„ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€í™” (íŒŒë€ìƒ‰ -> ë…¹ìƒ‰ -> ë¹¨ê°„ìƒ‰)
            ratio = i / num_points
            if ratio < 0.5:
                # íŒŒë€ìƒ‰ -> ë…¹ìƒ‰
                colors[i] = [0, ratio * 2, 1 - ratio * 2]
            else:
                # ë…¹ìƒ‰ -> ë¹¨ê°„ìƒ‰
                colors[i] = [(ratio - 0.5) * 2, 1 - (ratio - 0.5) * 2, 0]
        
        self.gt_pcd.colors = o3d.utility.Vector3dVector(colors)
        
        self.icp_pcd = o3d.geometry.PointCloud()
        self.icp_pcd.points = o3d.utility.Vector3dVector(estimated_points_downsampled)
        
        # ICP í¬ì¸íŠ¸ í´ë¼ìš°ë“œë„ ë™ì¼í•œ ê·¸ë¼ë°ì´ì…˜ ì ìš©
        num_points = len(estimated_points_downsampled)
        colors = np.zeros((num_points, 3))
        for i in range(num_points):
            ratio = i / num_points
            if ratio < 0.5:
                colors[i] = [0, ratio * 2, 1 - ratio * 2]
            else:
                colors[i] = [(ratio - 0.5) * 2, 1 - (ratio - 0.5) * 2, 0]
        
        self.icp_pcd.colors = o3d.utility.Vector3dVector(colors)
        
        # ê²°ê³¼ ì €ì¥
        results_dir = Path('results') / self.date
        os.makedirs(results_dir, exist_ok=True)
        
        # GT í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì €ì¥
        gt_filepath = results_dir / 'kitti_gt_pointcloud.pcd'
        o3d.io.write_point_cloud(str(gt_filepath), self.gt_pcd)
        print(f"âœ… GT í¬ì¸íŠ¸ í´ë¼ìš°ë“œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {gt_filepath}")
        
        # ICP í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì €ì¥
        icp_filepath = results_dir / 'kitti_estimated_pointcloud.pcd'
        o3d.io.write_point_cloud(str(icp_filepath), self.icp_pcd)
        print(f"âœ… ì¶”ì •ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {icp_filepath}")
        
        # ê¶¤ì  ë°ì´í„° ì €ì¥
        trajectory_data = {
            'gt_trajectory': np.array([self.get_pose_from_precomputed(i, pose_type='gt')[:3, 3] 
                                     for i in range(max_available_frames)
                                     if self.get_pose_from_precomputed(i, pose_type='gt') is not None]),
            'icp_trajectory': np.array([self.get_pose_from_precomputed(i, pose_type='icp')[:3, 3] 
                                      for i in range(max_available_frames)
                                      if self.get_pose_from_precomputed(i, pose_type='icp') is not None])
        }
        trajectory_filepath = results_dir / 'kitti_trajectory.npz'
        np.savez(trajectory_filepath, **trajectory_data)
        print(f"âœ… ê¶¤ì  ë°ì´í„°ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {trajectory_filepath}")
        
        print("í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìµœì í™” ì™„ë£Œ!")
        
        return gt_points_downsampled, estimated_points_downsampled
    
    def visualize_comparison_2d(self):
        """GT vs ì¶”ì •ëœ pose ë¹„êµ ì‹œê°í™”"""
        if not hasattr(self, 'gt_pcd') or not hasattr(self, 'icp_pcd'):
            print("âŒ ë¨¼ì € create_test_map_simple() í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•´ì„œ í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return
        
        if not hasattr(self, 'frame_poses_gt') or not hasattr(self, 'frame_poses_icp'):
            print("âŒ pose ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return
        
        print("2D ì‹œê°í™” ì‹œì‘...")
        
        fig, axes = plt.subplots(1, 3, figsize=(24, 8))
        
        # 1. GT pose ê¸°ë°˜ ì •í•©
        gt_points = np.asarray(self.gt_pcd.points)
        gt_colors = np.asarray(self.gt_pcd.colors)
        gt_trajectory = np.array([pose[:3, 3] for pose in self.frame_poses_gt])
        
        # í¬ì¸íŠ¸ ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¶”ê°€ë¡œ ìƒ˜í”Œë§
        if len(gt_points) > 50000:
            sample_indices = np.random.choice(len(gt_points), 50000, replace=False)
            gt_points_vis = gt_points[sample_indices]
            gt_colors_vis = gt_colors[sample_indices]
            print(f"GT í¬ì¸íŠ¸ ì‹œê°í™”ìš© ìƒ˜í”Œë§: {len(gt_points):,} â†’ {len(gt_points_vis):,}")
        else:
            gt_points_vis = gt_points
            gt_colors_vis = gt_colors
        
        axes[0].scatter(gt_points_vis[:, 0], gt_points_vis[:, 1], c=gt_colors_vis, s=0.1, alpha=0.7)
        axes[0].plot(gt_trajectory[:, 0], gt_trajectory[:, 1], 'r-', linewidth=3, label='GT Trajectory', alpha=0.9)
        axes[0].scatter(gt_trajectory[0, 0], gt_trajectory[0, 1], c='green', s=150, marker='o', label='Start', zorder=5)
        axes[0].scatter(gt_trajectory[-1, 0], gt_trajectory[-1, 1], c='red', s=150, marker='X', label='End', zorder=5)
        axes[0].set_title('GT Pose-based Mapping (Ground Truth)', fontsize=14, fontweight='bold')
        axes[0].set_xlabel('X Position (m)')
        axes[0].set_ylabel('Y Position (m)')
        axes[0].grid(True, alpha=0.3)
        axes[0].axis('equal')
        axes[0].legend()
        
        # 2. ì¶”ì •ëœ pose ê¸°ë°˜ ì •í•©
        estimated_points = np.asarray(self.icp_pcd.points)
        estimated_colors = np.asarray(self.icp_pcd.colors)
        estimated_trajectory = np.array([pose[:3, 3] for pose in self.frame_poses_icp])
        
        # í¬ì¸íŠ¸ ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì¶”ê°€ë¡œ ìƒ˜í”Œë§
        if len(estimated_points) > 50000:
            sample_indices = np.random.choice(len(estimated_points), 50000, replace=False)
            estimated_points_vis = estimated_points[sample_indices]
            estimated_colors_vis = estimated_colors[sample_indices]
            print(f"ì¶”ì • í¬ì¸íŠ¸ ì‹œê°í™”ìš© ìƒ˜í”Œë§: {len(estimated_points):,} â†’ {len(estimated_points_vis):,}")
        else:
            estimated_points_vis = estimated_points
            estimated_colors_vis = estimated_colors
        
        pose_method = "LIO (ICP) + GPS/IMU Fusion" if self.precomputed_poses else "Real-time ICP"
        
        axes[1].scatter(estimated_points_vis[:, 0], estimated_points_vis[:, 1], c=estimated_colors_vis, s=0.1, alpha=0.7)
        axes[1].plot(estimated_trajectory[:, 0], estimated_trajectory[:, 1], 'b-', linewidth=3, 
                    label=f'{pose_method} Trajectory', alpha=0.9)
        axes[1].scatter(estimated_trajectory[0, 0], estimated_trajectory[0, 1], c='green', s=150, marker='o', label='Start', zorder=5)
        axes[1].scatter(estimated_trajectory[-1, 0], estimated_trajectory[-1, 1], c='red', s=150, marker='X', label='End', zorder=5)
        
        # í‚¤í”„ë ˆì„ í‘œì‹œ (ì‹¤ì‹œê°„ ICPì¸ ê²½ìš°ì—ë§Œ)
        if not self.precomputed_poses and hasattr(self, 'keyframes') and self.keyframes:
            keyframe_positions = np.array([pose[:3, 3] for pose in self.keyframe_poses])
            axes[1].scatter(keyframe_positions[:, 0], keyframe_positions[:, 1], 
                           c='orange', s=100, marker='D', label='Keyframes', zorder=4, alpha=0.8)
        
        axes[1].set_title(f'{pose_method} Mapping', fontsize=14, fontweight='bold')
        axes[1].set_xlabel('X Position (m)')
        axes[1].set_ylabel('Y Position (m)')
        axes[1].grid(True, alpha=0.3)
        axes[1].axis('equal')
        axes[1].legend()
        
        # 3. ê¶¤ì  ë¹„êµ + ì˜¤ì°¨
        axes[2].plot(gt_trajectory[:, 0], gt_trajectory[:, 1], 'r-', linewidth=4, label='GT Trajectory', alpha=0.8)
        axes[2].plot(estimated_trajectory[:, 0], estimated_trajectory[:, 1], 'b-', linewidth=4, 
                    label=f'{pose_method} Trajectory', alpha=0.8)
        
        # ì˜¤ì°¨ ë²¡í„° í‘œì‹œ (ì¼ë¶€ë§Œ)
        step_error = max(1, len(gt_trajectory) // 20)
        for i in range(0, len(gt_trajectory), step_error):
            if i < len(estimated_trajectory):  # ì•ˆì „ì„± ì²´í¬
                axes[2].plot([gt_trajectory[i, 0], estimated_trajectory[i, 0]], 
                            [gt_trajectory[i, 1], estimated_trajectory[i, 1]], 
                            'k-', alpha=0.3, linewidth=1)
        
        axes[2].scatter(gt_trajectory[0, 0], gt_trajectory[0, 1], c='green', s=200, marker='o', label='Start', zorder=5)
        axes[2].scatter(gt_trajectory[-1, 0], gt_trajectory[-1, 1], c='red', s=200, marker='X', label='End', zorder=5)
        
        # ì˜¤ì°¨ í†µê³„ í‘œì‹œ (ê¶¤ì  ê¸¸ì´ë¥¼ ë§ì¶¤)
        min_length = min(len(gt_trajectory), len(estimated_trajectory))
        trajectory_errors = np.linalg.norm(gt_trajectory[:min_length] - estimated_trajectory[:min_length], axis=1)
        mean_error = np.mean(trajectory_errors)
        max_error = np.max(trajectory_errors)
        
        axes[2].text(0.02, 0.98, f'Mean Error: {mean_error:.2f}m\nMax Error: {max_error:.2f}m', 
                    transform=axes[2].transAxes, fontsize=12, verticalalignment='top',
                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        
        axes[2].set_title('Trajectory Comparison & Error Analysis', fontsize=14, fontweight='bold')
        axes[2].set_xlabel('X Position (m)')
        axes[2].set_ylabel('Y Position (m)')
        axes[2].grid(True, alpha=0.3)
        axes[2].axis('equal')
        axes[2].legend()
        
        plt.tight_layout()
        
        # ê²°ê³¼ ì €ì¥
        results_dir = Path('results') / self.date
        os.makedirs(results_dir, exist_ok=True)
        
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        filename = f'kitti_point_cloud_comparison_2d_{timestamp}.png'
        filepath = results_dir / filename
        
        print("ê·¸ë˜í”„ ì €ì¥ ì¤‘...")
        try:
            plt.savefig(filepath, dpi=150, bbox_inches='tight')  # DPIë¥¼ ë‚®ì¶°ì„œ ì„±ëŠ¥ ê°œì„ 
            print(f"âœ… 2D ë¹„êµ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
        except Exception as e:
            print(f"âš ï¸ ê·¸ë˜í”„ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print("ê·¸ë˜í”„ í‘œì‹œ ì¤‘...")
        plt.show()
    
    def analyze_improved_performance(self):
        """ì„±ëŠ¥ ë¶„ì„ (ì‚¬ì „ê³„ì‚° vs ì‹¤ì‹œê°„)"""
        if not hasattr(self, 'gt_pcd'):
            print("ë¨¼ì € í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return
        
        # ê¶¤ì  ì˜¤ì°¨ ê³„ì‚°
        gt_trajectory = np.array([pose[:3, 3] for pose in self.frame_poses_gt])
        estimated_trajectory = np.array([pose[:3, 3] for pose in self.frame_poses_icp])
        
        trajectory_errors = np.linalg.norm(gt_trajectory - estimated_trajectory, axis=1)
        
        # í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í†µê³„
        gt_points = np.asarray(self.gt_pcd.points)
        estimated_points = np.asarray(self.icp_pcd.points)
        
        # ë°©ë²• ê²°ì •
        method_name = "LIO (ICP) + GPS/IMU Fusion" if self.precomputed_poses else "Real-time ICP"
        
        # ë¦¬í¬íŠ¸ ì¶œë ¥
        print("\n" + "="*85)
        print("                    POINT CLOUD MAPPING PERFORMANCE REPORT")
        print("="*85)
        
        print("DATASET INFORMATION:")
        print(f"  â€¢ Total Frames Processed:       {len(self.frame_poses_gt)}")
        print(f"  â€¢ Total Points:                 {self.total_points:,}")
        if not self.precomputed_poses:
            print(f"  â€¢ Keyframes Generated:          {len(self.keyframes)}")
        print(f"  â€¢ Processing Method:            GT vs {method_name}")
        print(f"  â€¢ Pose Source:                  {'LIO (ICP) + GPS/IMU Fusion' if self.precomputed_poses else 'Real-time ICP'}")
        
        print("-"*85)
        print("POINT CLOUD STATISTICS:")
        print(f"  â€¢ GT Aligned Points:            {len(gt_points):,}")
        print(f"  â€¢ Estimated Aligned Points:     {len(estimated_points):,}")
        print(f"  â€¢ Map Consistency:              {abs(len(gt_points) - len(estimated_points)) / len(gt_points) * 100:.1f}% difference")
        
        print("-"*85)
        print("TRAJECTORY ACCURACY ANALYSIS:")
        print(f"  â€¢ Mean Trajectory Error:        {np.mean(trajectory_errors):.3f} m")
        print(f"  â€¢ Median Trajectory Error:      {np.median(trajectory_errors):.3f} m")
        print(f"  â€¢ Max Trajectory Error:         {np.max(trajectory_errors):.3f} m")
        print(f"  â€¢ Min Trajectory Error:         {np.min(trajectory_errors):.3f} m")
        print(f"  â€¢ RMSE Trajectory Error:        {np.sqrt(np.mean(trajectory_errors**2)):.3f} m")
        print(f"  â€¢ Standard Deviation:           {np.std(trajectory_errors):.3f} m")
        
        # ì˜¤ì°¨ ë°±ë¶„ìœ„ìˆ˜
        p25 = np.percentile(trajectory_errors, 25)
        p75 = np.percentile(trajectory_errors, 75)
        p95 = np.percentile(trajectory_errors, 95)
        
        print(f"  â€¢ 25th Percentile Error:        {p25:.3f} m")
        print(f"  â€¢ 75th Percentile Error:        {p75:.3f} m")
        print(f"  â€¢ 95th Percentile Error:        {p95:.3f} m")
        
        # GT vs ì¶”ì • ì´ ì´ë™ ê±°ë¦¬ ë¹„êµ
        gt_total_distance = np.sum(np.linalg.norm(np.diff(gt_trajectory, axis=0), axis=1))
        estimated_total_distance = np.sum(np.linalg.norm(np.diff(estimated_trajectory, axis=0), axis=1))
        
        print("-"*85)
        print("DISTANCE ANALYSIS:")
        print(f"  â€¢ GT Total Distance:            {gt_total_distance:.1f} m")
        print(f"  â€¢ Estimated Total Distance:     {estimated_total_distance:.1f} m")
        print(f"  â€¢ Distance Difference:          {abs(gt_total_distance - estimated_total_distance):.1f} m")
        print(f"  â€¢ Distance Accuracy:            {100 - abs(gt_total_distance - estimated_total_distance)/gt_total_distance*100:.1f}%")
        
        print("-"*85)
        if self.precomputed_poses:
            print("PRE-COMPUTED POSE FEATURES:")
            print("  â€¢ Algorithm Used:               LIO (ICP) + GPS/IMU Sensor Fusion")
            print("  â€¢ Kalman Filter:                âœ“ Extended Kalman Filter")
            print("  â€¢ Real-time Processing:         âœ“ Pre-computed offline")
            print("  â€¢ Computational Efficiency:     âœ“ High (loading from file)")
            print("  â€¢ Consistency:                  âœ“ Identical results every run")
        else:
            print("REAL-TIME ICP FEATURES:")
            print("  â€¢ Multi-scale ICP:              âœ“ Coarse-to-fine registration")
            print("  â€¢ Feature-based Initialization: âœ“ FPFH features + RANSAC")
            print("  â€¢ Statistical Outlier Removal:  âœ“ Advanced preprocessing")
            print("  â€¢ Loop Closure Detection:       âœ“ Feature matching based")
            print("  â€¢ Motion Validation:            âœ“ Unrealistic motion filtering")
        
        print("-"*85)
        print("PERFORMANCE ASSESSMENT:")
        
        # í’ˆì§ˆ í‰ê°€
        mean_error = np.mean(trajectory_errors)
        if mean_error < 2.0:
            quality_grade = "EXCELLENT"
            description = "Commercial-grade accuracy"
        elif mean_error < 5.0:
            quality_grade = "VERY GOOD"
            description = "Research-grade system"
        elif mean_error < 10.0:
            quality_grade = "GOOD"
            description = "Acceptable for most applications"
        elif mean_error < 20.0:
            quality_grade = "FAIR"
            description = "Needs further optimization"
        else:
            quality_grade = "POOR"
            description = "Significant improvements required"
        
        print(f"  â€¢ Overall Quality Grade:        {quality_grade}")
        print(f"  â€¢ System Assessment:            {description}")
        print(f"  â€¢ Drift Rate:                   {mean_error/gt_total_distance*100:.3f}% per meter")
        print(f"  â€¢ Suitable for Autonomous Nav:  {'YES' if mean_error < 5.0 else 'NO'}")
        print(f"  â€¢ Suitable for Mapping:         {'YES' if mean_error < 10.0 else 'NO'}")
        
        print("-"*85)
        print("IMPROVEMENT RECOMMENDATIONS:")
        if self.precomputed_poses:
            if mean_error > 2.0:
                print("  â€¢ Consider improving the pose estimation algorithm")
                print("  â€¢ Add more sophisticated sensor fusion techniques")
            print("  â€¢ Advantage: Consistent and reproducible results")
            print("  â€¢ Advantage: Fast mapping execution")
        else:
            if mean_error > 5.0:
                print("  â€¢ Consider adding IMU integration for better motion prediction")
                print("  â€¢ Implement more sophisticated loop closure detection")
                print("  â€¢ Add bundle adjustment for global optimization")
            if len(self.keyframes) < 10:
                print("  â€¢ Increase keyframe frequency for better loop closure")
            if np.std(trajectory_errors) > mean_error:
                print("  â€¢ High error variance - consider adaptive parameters")
        
        print("="*85)
        print(f"Point Cloud Mapping with {method_name} completed successfully!")
        print("="*85)
        
        # results í´ë” ìƒì„±
        results_dir = 'results'
        os.makedirs(results_dir, exist_ok=True)
        
        # ì„±ëŠ¥ ë¦¬í¬íŠ¸ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        method_name = "lio_gps_imu" if self.precomputed_poses else "realtime_icp"
        report_filename = f'kitti_mapping_report_{method_name}_{timestamp}.txt'
        report_filepath = os.path.join(results_dir, report_filename)
        
        # í˜„ì¬ ì¶œë ¥ì„ íŒŒì¼ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸
        import sys
        original_stdout = sys.stdout
        with open(report_filepath, 'w', encoding='utf-8') as f:
            sys.stdout = f
            # ë¦¬í¬íŠ¸ ë‚´ìš©ì„ ì§ì ‘ íŒŒì¼ì— ì‘ì„±
            f.write("\n" + "="*85 + "\n")
            f.write("                    POINT CLOUD MAPPING PERFORMANCE REPORT\n")
            f.write("="*85 + "\n\n")
            
            f.write("DATASET INFORMATION:\n")
            f.write(f"  â€¢ Total Frames Processed:       {len(self.frame_poses_gt)}\n")
            f.write(f"  â€¢ Total Points:                 {self.total_points:,}\n")
            if not self.precomputed_poses:
                f.write(f"  â€¢ Keyframes Generated:          {len(self.keyframes)}\n")
            f.write(f"  â€¢ Processing Method:            GT vs {method_name}\n")
            f.write(f"  â€¢ Pose Source:                  {'LIO (ICP) + GPS/IMU Fusion' if self.precomputed_poses else 'Real-time ICP'}\n")
            
            f.write("-"*85 + "\n")
            f.write("POINT CLOUD STATISTICS:\n")
            f.write(f"  â€¢ GT Aligned Points:            {len(gt_points):,}\n")
            f.write(f"  â€¢ Estimated Aligned Points:     {len(estimated_points):,}\n")
            f.write(f"  â€¢ Map Consistency:              {abs(len(gt_points) - len(estimated_points)) / len(gt_points) * 100:.1f}% difference\n")
            
            f.write("-"*85 + "\n")
            f.write("TRAJECTORY ACCURACY ANALYSIS:\n")
            f.write(f"  â€¢ Mean Trajectory Error:        {np.mean(trajectory_errors):.3f} m\n")
            f.write(f"  â€¢ Median Trajectory Error:      {np.median(trajectory_errors):.3f} m\n")
            f.write(f"  â€¢ Max Trajectory Error:         {np.max(trajectory_errors):.3f} m\n")
            f.write(f"  â€¢ Min Trajectory Error:         {np.min(trajectory_errors):.3f} m\n")
            f.write(f"  â€¢ RMSE Trajectory Error:        {np.sqrt(np.mean(trajectory_errors**2)):.3f} m\n")
            f.write(f"  â€¢ Standard Deviation:           {np.std(trajectory_errors):.3f} m\n")
            
            # ì˜¤ì°¨ ë°±ë¶„ìœ„ìˆ˜
            f.write("-"*85 + "\n")
            f.write("TRAJECTORY ACCURACY ANALYSIS:\n")
            f.write(f"  â€¢ 25th Percentile Error:        {p25:.3f} m\n")
            f.write(f"  â€¢ 75th Percentile Error:        {p75:.3f} m\n")
            f.write(f"  â€¢ 95th Percentile Error:        {p95:.3f} m\n")
            
            f.write("-"*85 + "\n")
            f.write("DISTANCE ANALYSIS:\n")
            f.write(f"  â€¢ GT Total Distance:            {gt_total_distance:.1f} m\n")
            f.write(f"  â€¢ Estimated Total Distance:     {estimated_total_distance:.1f} m\n")
            f.write(f"  â€¢ Distance Difference:          {abs(gt_total_distance - estimated_total_distance):.1f} m\n")
            f.write(f"  â€¢ Distance Accuracy:            {100 - abs(gt_total_distance - estimated_total_distance)/gt_total_distance*100:.1f}%\n")
            
            f.write("-"*85 + "\n")
            if self.precomputed_poses:
                f.write("PRE-COMPUTED POSE FEATURES:\n")
                f.write("  â€¢ Algorithm Used:               LIO (ICP) + GPS/IMU Sensor Fusion\n")
                f.write("  â€¢ Kalman Filter:                âœ“ Extended Kalman Filter\n")
                f.write("  â€¢ Real-time Processing:         âœ“ Pre-computed offline\n")
                f.write("  â€¢ Computational Efficiency:     âœ“ High (loading from file)\n")
                f.write("  â€¢ Consistency:                  âœ“ Identical results every run\n")
            else:
                f.write("REAL-TIME ICP FEATURES:\n")
                f.write("  â€¢ Multi-scale ICP:              âœ“ Coarse-to-fine registration\n")
                f.write("  â€¢ Feature-based Initialization: âœ“ FPFH features + RANSAC\n")
                f.write("  â€¢ Statistical Outlier Removal:  âœ“ Advanced preprocessing\n")
                f.write("  â€¢ Loop Closure Detection:       âœ“ Feature matching based\n")
                f.write("  â€¢ Motion Validation:            âœ“ Unrealistic motion filtering\n")
            
            f.write("-"*85 + "\n")
            f.write("PERFORMANCE ASSESSMENT:\n")
            
            # í’ˆì§ˆ í‰ê°€
            f.write(f"  â€¢ Overall Quality Grade:        {quality_grade}\n")
            f.write(f"  â€¢ System Assessment:            {description}\n")
            f.write(f"  â€¢ Drift Rate:                   {mean_error/gt_total_distance*100:.3f}% per meter\n")
            f.write(f"  â€¢ Suitable for Autonomous Nav:  {'YES' if mean_error < 5.0 else 'NO'}\n")
            f.write(f"  â€¢ Suitable for Mapping:         {'YES' if mean_error < 10.0 else 'NO'}\n")
            
            f.write("-"*85 + "\n")
            f.write("IMPROVEMENT RECOMMENDATIONS:\n")
            if self.precomputed_poses:
                if mean_error > 2.0:
                    f.write("  â€¢ Consider improving the pose estimation algorithm\n")
                    f.write("  â€¢ Add more sophisticated sensor fusion techniques\n")
                f.write("  â€¢ Advantage: Consistent and reproducible results\n")
                f.write("  â€¢ Advantage: Fast mapping execution\n")
            else:
                if mean_error > 5.0:
                    f.write("  â€¢ Consider adding IMU integration for better motion prediction\n")
                    f.write("  â€¢ Implement more sophisticated loop closure detection\n")
                    f.write("  â€¢ Add bundle adjustment for global optimization\n")
                if len(self.keyframes) < 10:
                    f.write("  â€¢ Increase keyframe frequency for better loop closure\n")
                if np.std(trajectory_errors) > mean_error:
                    f.write("  â€¢ High error variance - consider adaptive parameters\n")
            
            f.write("="*85 + "\n")
            f.write(f"Point Cloud Mapping with {method_name} completed successfully!\n")
            f.write("="*85 + "\n")
            
            sys.stdout = original_stdout
        
        print(f"\nì„±ëŠ¥ ë¦¬í¬íŠ¸ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {report_filepath}")

    def debug_pose_and_transformation(self, frame_idx, points, pose_matrix, method_name):
        """poseì™€ ë³€í™˜ ê²°ê³¼ ë””ë²„ê¹…"""
        print(f"\n=== {method_name} - í”„ë ˆì„ {frame_idx} ë””ë²„ê¹… ===")
        print(f"Pose í–‰ë ¬:")
        print(pose_matrix)
        print(f"ìœ„ì¹˜: {pose_matrix[:3, 3]}")
        
        # ëª‡ ê°œ í¬ì¸íŠ¸ë§Œ ë³€í™˜í•´ì„œ í™•ì¸
        sample_points = points[:5]  # ì²« 5ê°œ í¬ì¸íŠ¸
        transformed_points = self.transform_points_to_world(sample_points, pose_matrix)
        
        print(f"ì›ë³¸ í¬ì¸íŠ¸ (ì²« 5ê°œ):")
        for i, p in enumerate(sample_points):
            print(f"  {i}: [{p[0]:.3f}, {p[1]:.3f}, {p[2]:.3f}]")
        
        print(f"ë³€í™˜ëœ í¬ì¸íŠ¸ (ì²« 5ê°œ):")
        for i, p in enumerate(transformed_points):
            print(f"  {i}: [{p[0]:.3f}, {p[1]:.3f}, {p[2]:.3f}]")
        
        print(f"ë³€í™˜ëŸ‰: {np.linalg.norm(pose_matrix[:3, 3]):.3f}m")
        print("=" * 50)
    
    def create_test_map_simple(self, max_frames=None):
        """ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ë§µ ìƒì„±"""
        print("ğŸ” í¬ì¸íŠ¸í´ë¼ìš°ë“œ ì •í•© ë¬¸ì œ ì§„ë‹¨ ì‹œì‘...")
        
        # í”„ë ˆì„ ë²”ìœ„ ì„¤ì •
        if max_frames is None:
            max_frames = len(self.velodyne_files)
        
        # ì‚¬ì „ ê³„ì‚°ëœ pose ë°ì´í„°ì˜ ë²”ìœ„ í™•ì¸
        if self.precomputed_poses:
            max_gt_frames = len(self.precomputed_poses.get('gt_poses', []))
            max_icp_frames = len(self.precomputed_poses.get('fused_poses', []))
            max_available_frames = min(max_gt_frames, max_icp_frames, max_frames, len(self.velodyne_files))
            print(f"  â€¢ ì²˜ë¦¬ ê°€ëŠ¥í•œ ìµœëŒ€ í”„ë ˆì„: {max_available_frames}")
        
        # í”„ë ˆì„ë³„ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ì €ì¥
        frame_points = []
        frame_poses_gt = []
        frame_poses_icp = []
        
        # 2í”„ë ˆì„ ê°„ê²©ìœ¼ë¡œ ì²˜ë¦¬
        for frame_idx in range(0, max_frames, 2):
            # í˜„ì¬ í”„ë ˆì„ì˜ ë¼ì´ë‹¤ í¬ì¸íŠ¸ ë¡œë“œ
            points = self.load_velodyne_points(self.velodyne_files[frame_idx])
            
            # GT poseì™€ ICP pose ê°€ì ¸ì˜¤ê¸°
            gt_pose = self.get_pose_from_precomputed(frame_idx, pose_type='gt')
            icp_pose = self.get_pose_from_precomputed(frame_idx, pose_type='icp')
            
            if gt_pose is not None and icp_pose is not None:
                # í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ì›”ë“œ ì¢Œí‘œê³„ë¡œ ë³€í™˜
                gt_points = self.transform_points_to_world(points, gt_pose)
                icp_points = self.transform_points_to_world(points, icp_pose)
                
                frame_points.append((gt_points, icp_points))
                frame_poses_gt.append(gt_pose)
                frame_poses_icp.append(icp_pose)
            else:
                print(f"âš ï¸ í”„ë ˆì„ {frame_idx}ì˜ pose ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        if not frame_points:
            print("âŒ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # í¬ì¸íŠ¸ í´ë¼ìš°ë“œ í†µí•©
        gt_points = np.vstack([points[0] for points in frame_points])
        icp_points = np.vstack([points[1] for points in frame_points])
        
        # ë‹¤ìš´ìƒ˜í”Œë§ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
        voxel_size = 0.2
        print(f"ë‹¤ìš´ìƒ˜í”Œë§ ì‹œì‘ (ë³µì…€ í¬ê¸°: {voxel_size}m)...")
        
        # GT í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë‹¤ìš´ìƒ˜í”Œë§
        gt_pcd_temp = o3d.geometry.PointCloud()
        gt_pcd_temp.points = o3d.utility.Vector3dVector(gt_points[:, :3])
        gt_pcd_temp = gt_pcd_temp.voxel_down_sample(voxel_size)
        gt_points_downsampled = np.asarray(gt_pcd_temp.points)
        
        # ICP í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë‹¤ìš´ìƒ˜í”Œë§
        icp_pcd_temp = o3d.geometry.PointCloud()
        icp_pcd_temp.points = o3d.utility.Vector3dVector(icp_points[:, :3])
        icp_pcd_temp = icp_pcd_temp.voxel_down_sample(voxel_size)
        icp_points_downsampled = np.asarray(icp_pcd_temp.points)
        
        print(f"ë‹¤ìš´ìƒ˜í”Œë§ ì™„ë£Œ:")
        print(f"  â€¢ GT í¬ì¸íŠ¸: {len(gt_points):,} â†’ {len(gt_points_downsampled):,}")
        print(f"  â€¢ ICP í¬ì¸íŠ¸: {len(icp_points):,} â†’ {len(icp_points_downsampled):,}")
        
        # Open3D í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± (ë‹¤ìš´ìƒ˜í”Œë§ëœ ë²„ì „)
        self.gt_pcd = o3d.geometry.PointCloud()
        self.gt_pcd.points = o3d.utility.Vector3dVector(gt_points_downsampled)
        
        # ì‹œê°„ì— ë”°ë¥¸ ê·¸ë¼ë°ì´ì…˜ ìƒ‰ìƒ ìƒì„±
        num_points = len(gt_points_downsampled)
        colors = np.zeros((num_points, 3))
        for i in range(num_points):
            # ì‹œê°„ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€í™” (íŒŒë€ìƒ‰ -> ë…¹ìƒ‰ -> ë¹¨ê°„ìƒ‰)
            ratio = i / num_points
            if ratio < 0.5:
                # íŒŒë€ìƒ‰ -> ë…¹ìƒ‰
                colors[i] = [0, ratio * 2, 1 - ratio * 2]
            else:
                # ë…¹ìƒ‰ -> ë¹¨ê°„ìƒ‰
                colors[i] = [(ratio - 0.5) * 2, 1 - (ratio - 0.5) * 2, 0]
        
        self.gt_pcd.colors = o3d.utility.Vector3dVector(colors)
        
        self.icp_pcd = o3d.geometry.PointCloud()
        self.icp_pcd.points = o3d.utility.Vector3dVector(icp_points_downsampled)
        
        # ICP í¬ì¸íŠ¸ í´ë¼ìš°ë“œë„ ë™ì¼í•œ ê·¸ë¼ë°ì´ì…˜ ì ìš©
        num_points = len(icp_points_downsampled)
        colors = np.zeros((num_points, 3))
        for i in range(num_points):
            ratio = i / num_points
            if ratio < 0.5:
                colors[i] = [0, ratio * 2, 1 - ratio * 2]
            else:
                colors[i] = [(ratio - 0.5) * 2, 1 - (ratio - 0.5) * 2, 0]
        
        self.icp_pcd.colors = o3d.utility.Vector3dVector(colors)
        
        # pose ì €ì¥
        self.frame_poses_gt = frame_poses_gt
        self.frame_poses_icp = frame_poses_icp
        
        print("âœ… í…ŒìŠ¤íŠ¸ ë§µ ìƒì„± ì™„ë£Œ!")
        print(f"  â€¢ ì²˜ë¦¬ëœ í”„ë ˆì„ ìˆ˜: {len(frame_points)}")
        print(f"  â€¢ GT í¬ì¸íŠ¸ ìˆ˜: {len(gt_points_downsampled):,}")
        print(f"  â€¢ ICP í¬ì¸íŠ¸ ìˆ˜: {len(icp_points_downsampled):,}")
        
        # í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë²”ìœ„ ì¶œë ¥
        gt_min = np.min(gt_points_downsampled, axis=0)
        gt_max = np.max(gt_points_downsampled, axis=0)
        print("\ní¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë²”ìœ„:")
        print(f"  â€¢ X ë²”ìœ„: [{gt_min[0]:.2f}, {gt_max[0]:.2f}]")
        print(f"  â€¢ Y ë²”ìœ„: [{gt_min[1]:.2f}, {gt_max[1]:.2f}]")
        print(f"  â€¢ Z ë²”ìœ„: [{gt_min[2]:.2f}, {gt_max[2]:.2f}]")

    def visualize_camera_lidar_fusion(self, start_frame=0, num_frames=5, frame_interval=10):
        """ì¹´ë©”ë¼-ë¼ì´ë‹¤ ì •í•© ì‹œê°í™” (ì—¬ëŸ¬ í”„ë ˆì„)"""
        print("\nâœ… ì¹´ë©”ë¼-ë¼ì´ë‹¤ ì •í•© ì‹œê°í™” ì‹œì‘...")
        
        # 1. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒì¼ ë¡œë“œ
        calib_path = os.path.join(self.base_path, self.date, 'calib_cam_to_cam.txt')
        try:
            with open(calib_path, 'r') as f:
                calib_data = f.readlines()
            
            # ìº˜ë¦¬ë¸Œë ˆì´ì…˜ í–‰ë ¬ íŒŒì‹±
            # P2 (ì¹´ë©”ë¼ í”„ë¡œì ì…˜ í–‰ë ¬)
            P2_line = [line for line in calib_data if line.startswith('P_rect_02')][0]
            P2 = np.array([float(x) for x in P2_line.strip().split()[1:13]]).reshape(3, 4)
            
            # R0_rect (ì •ê·œí™” í–‰ë ¬)
            R0_line = [line for line in calib_data if line.startswith('R_rect_02')][0]
            R0_rect = np.array([float(x) for x in R0_line.strip().split()[1:10]]).reshape(3, 3)
            
            # ë¼ì´ë‹¤->ì¹´ë©”ë¼ ë³€í™˜ í–‰ë ¬ ë¡œë“œ
            velo_to_cam_path = os.path.join(self.base_path, self.date, 'calib_velo_to_cam.txt')
            with open(velo_to_cam_path, 'r') as f:
                velo_to_cam_data = f.readlines()
            
            # R (íšŒì „ í–‰ë ¬)
            R_line = [line for line in velo_to_cam_data if line.startswith('R:')][0]
            R = np.array([float(x) for x in R_line.strip().split()[1:10]]).reshape(3, 3)
            
            # T (ì´ë™ ë²¡í„°)
            T_line = [line for line in velo_to_cam_data if line.startswith('T:')][0]
            T = np.array([float(x) for x in T_line.strip().split()[1:4]]).reshape(3, 1)
            
            # Tr_velo_to_cam ìƒì„± (3x4 ë³€í™˜ í–‰ë ¬)
            Tr_velo_to_cam = np.hstack((R, T))
            
            print("âœ… ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒì¼ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            print(f"âŒ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return
        
        # 2. ì—¬ëŸ¬ í”„ë ˆì„ ì‹œê°í™”
        for i in range(num_frames):
            frame_idx = start_frame + i * frame_interval
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image_path = os.path.join(self.base_path, self.drive, 'image_02', 'data', f'{frame_idx:010d}.png')
            try:
                image = plt.imread(image_path)
                print(f"âœ… ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: {image_path}")
            except Exception as e:
                print(f"âŒ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
            
            # ë¼ì´ë‹¤ í¬ì¸íŠ¸ ë¡œë“œ ë° í•„í„°ë§
            points = self.load_velodyne_points(self.velodyne_files[frame_idx])
            filtered_points = self.filter_point_cloud(points)
            
            # í¬ì¸íŠ¸ í´ë¼ìš°ë“œë¥¼ ì¹´ë©”ë¼ ì¢Œí‘œê³„ë¡œ ë³€í™˜
            points_homo = np.ones((len(filtered_points), 4))
            points_homo[:, :3] = filtered_points[:, :3]
            
            # ë¼ì´ë‹¤ -> ì¹´ë©”ë¼ ë³€í™˜
            points_cam = (Tr_velo_to_cam @ points_homo.T).T
            
            # ì •ê·œí™”
            points_rect = (R0_rect @ points_cam[:, :3].T).T
            
            # ì¹´ë©”ë¼ ì¢Œí‘œë¥¼ ì´ë¯¸ì§€ í‰ë©´ì— íˆ¬ì˜
            points_proj = (P2 @ np.vstack((points_rect.T, np.ones(len(points_rect))))).T
            points_proj = points_proj[:, :2] / points_proj[:, 2:]
            
            # ì´ë¯¸ì§€ ë‚´ë¶€ì— ìˆëŠ” í¬ì¸íŠ¸ë§Œ ì„ íƒ
            mask = (points_proj[:, 0] >= 0) & (points_proj[:, 0] < image.shape[1]) & \
                   (points_proj[:, 1] >= 0) & (points_proj[:, 1] < image.shape[0]) & \
                   (points_cam[:, 2] > 0)  # ì¹´ë©”ë¼ ì•ìª½ í¬ì¸íŠ¸ë§Œ
            
            points_proj = points_proj[mask]
            points_cam = points_cam[mask]
            filtered_points = filtered_points[mask]
            
            # ì‹œê°í™”
            plt.figure(figsize=(15, 5))
            
            # ì›ë³¸ ì´ë¯¸ì§€
            plt.subplot(121)
            plt.imshow(image)
            plt.title(f'Original Image (Frame {frame_idx})', fontsize=12)
            plt.axis('off')
            
            # ë¼ì´ë‹¤ í¬ì¸íŠ¸ ì˜¤ë²„ë ˆì´
            plt.subplot(122)
            plt.imshow(image)
            
            # ê±°ë¦¬ì— ë”°ë¥¸ ìƒ‰ìƒ ë§¤í•‘
            distances = np.linalg.norm(points_cam[:, :3], axis=1)
            colors = plt.cm.jet(distances / np.max(distances))
            
            # í¬ì¸íŠ¸ í¬ê¸°ëŠ” ê±°ë¦¬ì— ë°˜ë¹„ë¡€
            sizes = 100 / (distances + 1)
            
            plt.scatter(points_proj[:, 0], points_proj[:, 1], c=colors, s=sizes, alpha=0.6)
            plt.title(f'LiDAR Points Overlay (Frame {frame_idx})', fontsize=12)
            plt.axis('off')
            
            plt.tight_layout()
            
            # ê²°ê³¼ ì €ì¥
            results_dir = Path('results') / self.date
            os.makedirs(results_dir, exist_ok=True)
            
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            filename = f'kitti_camera_lidar_fusion_frame{frame_idx}_{timestamp}.png'
            filepath = results_dir / filename
            plt.savefig(filepath, dpi=300, bbox_inches='tight')
            print(f"âœ… ì •í•© ì‹œê°í™” ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
            
            plt.show()
            
            # í†µê³„ ì¶œë ¥
            print(f"\nğŸ“Š í”„ë ˆì„ {frame_idx} ì •í•© í†µê³„:")
            print(f"  â€¢ ì´ ë¼ì´ë‹¤ í¬ì¸íŠ¸: {len(filtered_points):,}")
            print(f"  â€¢ ì´ë¯¸ì§€ì— íˆ¬ì˜ëœ í¬ì¸íŠ¸: {len(points_proj):,}")
            print(f"  â€¢ íˆ¬ì˜ ë¹„ìœ¨: {len(points_proj)/len(filtered_points)*100:.1f}%")
            print(f"  â€¢ ìµœì†Œ ê±°ë¦¬: {np.min(distances):.1f}m")
            print(f"  â€¢ ìµœëŒ€ ê±°ë¦¬: {np.max(distances):.1f}m")
            print(f"  â€¢ í‰ê·  ê±°ë¦¬: {np.mean(distances):.1f}m")
            print("-" * 50)

def main():
    parser = argparse.ArgumentParser(description='KITTI Point Cloud Mapping')
    parser.add_argument('--base_path', type=str, 
                      default='KITTI_dataset',
                      help='KITTI ë°ì´í„°ì…‹ì˜ ê¸°ë³¸ ê²½ë¡œ')
    parser.add_argument('--dataset_prefix', type=str,
                      default='2011_09_26_drive_0020',
                      help='ë°ì´í„°ì…‹ í”„ë¦¬í”½ìŠ¤ (ì˜ˆ: 2011_09_26_drive_0001)')
    parser.add_argument('--pose_file', type=str,
                      help='ì‚¬ì „ ê³„ì‚°ëœ pose íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: results/{dataset_prefix}/kitti_poses_*.pkl)')
    parser.add_argument('--max_frames', type=int,
                      default=None,
                      help='ì²˜ë¦¬í•  ìµœëŒ€ í”„ë ˆì„ ìˆ˜ (ê¸°ë³¸ê°’: ëª¨ë“  í”„ë ˆì„)')
    parser.add_argument('--voxel_size', type=float,
                      default=0.2,
                      help='í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ë‹¤ìš´ìƒ˜í”Œë§ì„ ìœ„í•œ ë³µì…€ í¬ê¸°')
    parser.add_argument('--output_dir', type=str,
                      default='results',
                      help='ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬')
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    results_dir = Path(args.output_dir) / args.dataset_prefix
    os.makedirs(results_dir, exist_ok=True)
    
    # pose íŒŒì¼ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
    if args.pose_file is None:
        pose_files = sorted(glob.glob(str(results_dir / 'kitti_poses_*.pkl')))
        if pose_files:
            args.pose_file = pose_files[-1]  # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì‚¬ìš©
            print(f"ìë™ìœ¼ë¡œ ìµœê·¼ pose íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤: {args.pose_file}")
        else:
            print("ê²½ê³ : pose íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # Point Cloud Mapper ì´ˆê¸°í™”
    mapper = ImprovedKITTIPointCloudMapper(args.base_path, args.dataset_prefix, args.pose_file)
    
    print("KITTI Point Cloud Mapping ì‹œì‘...")
    print(f"ë°ì´í„°ì…‹: {args.dataset_prefix}")
    
    # í…ŒìŠ¤íŠ¸ ë§µ ìƒì„±
    mapper.create_test_map_simple(max_frames=args.max_frames)
    
    # ìµœì í™”ëœ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„± ë° ì €ì¥
    gt_points, estimated_points = mapper.create_optimized_point_clouds(voxel_size=args.voxel_size)
    
    if gt_points is not None and estimated_points is not None:
        # ê²°ê³¼ ì‹œê°í™”
        mapper.visualize_comparison_2d()
        print(f"\nì™„ë£Œ! ê²°ê³¼ê°€ {results_dir} ë””ë ‰í† ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ í¬ì¸íŠ¸ í´ë¼ìš°ë“œ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 